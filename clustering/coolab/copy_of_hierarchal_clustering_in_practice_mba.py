# -*- coding: utf-8 -*-
"""Copy of Hierarchal clustering in practice-MBA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IUjzLMjaN3Du8N1EObvOBcSW5nA7wyOh

# Before you start

*  **You should create a copy of this Colab in your own drive by selecting: 
File -> Save a copy in Drive**

*  **Uplaod the file facilties.csv under  /content/sample_data/**

Note: When you load the Colab page, the **facilties.csv** file will be removed. Thus, you will need to upload it again.

# Code usage

This code performs the hierarchical clustering, using two inputs from your side:

1.   The number of clusters
2.   The clustering variables

In the following code cell, you should define the value of the two parameters mentioned above:
1.   Edit the parameter **n_clutsers** and insert the number of desired clusters. You may want to start with a number of clusters equal to 1 (only for demo purposes) and then keep incrementing it until you reach a suitable number of clusters (one that will be explained in your analysis).
2.   Select the relevant clustering variables by uncommenting them: you need to remove the # at the beginning of the respective line (e.g a clustering process that groups the facilities into four clusters by using the variables URBAN, MIN_AIR_2015, and MIN_AIR_2016 should look like the following code block. 


```
# n_clusters = 4
selected_clustering_variables = [
    # 'FACILITY_ID',
    # 'GOV',
    # 'DISTRICT',
    # 'FACILITY_TYPE',
     'URBAN',
     'MIN_AIR_2015',
     'MIN_AIR_2016',
    # 'MIN_GROUND_2015',
    # 'MIN_GROUND_2016',
    # 'ON_GRID',
    # 'ON_GRID_AVL',
    # 'SOLAR_ONL',
    # 'SOLAR_INC',
    # 'DIESEL_ONL',
    # 'DIESEL_INC'
]

```



***Reminder:***

**Please note that the clustering can be only performed on numerical values (floats) and booleans.**


**You should run the code cells from the top to the bottom, one by one. **


**Whenever you adjust 1. and/or 2., you should rerun all code cells again.**
**The code cells should be run in successive order. Also, you should not run a cell while another one is still running. **


# The code output

After running each code cell, you will have four useful outputs: 


*   The clustering dendrogram (an image that you can download).
*   The clustering scatter plot (an image that you can download).
*   A file **clustered_facilities** in CSV format located under  **content/sample_data**. You can download it and manipulate it in Excel. The file stores all the clustered facilities and contains a new column that indicates the cluster to which a facility belongs.
*   The last cell that you execute displays the size of each cluster. This is very relevant too.
"""

# 1. todo: define number of clusters:
n_clusters = 7
# 2. todo: remove the # to include a variable in the clustering
selected_clustering_variables = [
    'FACILITY_ID',
    'FACILITY_TYPE',
    'URBAN',
    'MIN_AIR_2015',
    'MIN_AIR_2016',
    'MIN_GROUND_2015',
    'MIN_GROUND_2016',
    'ON_GRID',
    'ON_GRID_AVL',
    'SOLAR_ONL',
    'SOLAR_INC',
    'DIESEL_ONL',
    # 'DIESEL_INC'
]

"""This code cell contains all the imports used in the following code, and it defines functions for visualization. You should run it."""

from sklearn.preprocessing import StandardScaler

# Import standard libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram
from scipy.cluster import hierarchy
import sys
sys.setrecursionlimit(2000)

# Import the hierarchical clustering algorithm
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler

import seaborn as sns

palette = sns.color_palette("bright", 10)

def display_factorial_planes(X_projected, n_comp, pca, axis_ranks, labels=None, alpha=1, illustrative_var=None):
    '''Display a scatter plot on a factorial plane, one for each factorial plane'''

    font = {'family': 'serif',
            'color': 'black',
            'weight': 'bold',
            'size': 12,
            }
    # For each factorial plane
    for d1, d2 in axis_ranks:
        if d2 < n_comp:

            # Initialise the matplotlib figure
            fig = plt.figure(figsize=(7.5, 7.5))

            # Display the points
            if illustrative_var is None:
                plt.scatter(X_projected[:, d1], X_projected[:, d2], alpha=alpha)
            else:
                illustrative_var = np.array(illustrative_var)
                for value in np.unique(illustrative_var):
                    selected = np.where(illustrative_var == value)
                    plt.scatter(X_projected[selected, d1], X_projected[selected, d2], alpha=alpha, label=value)
                plt.legend()

            # Display the labels on the points
            if labels is not None:
                for i, (x, y) in enumerate(X_projected[:, [d1, d2]]):
                    plt.text(x, y, labels[i],
                            fontdict=font, fontsize='8', ha='center', va='top')

                    # Define the limits of the chart
            boundary = np.max(np.abs(X_projected[:, [d1, d2]])) * 1.1
            plt.xlim([-boundary, boundary])
            plt.ylim([-boundary, boundary])

            # Display grid lines
            plt.plot([-100, 100], [0, 0], color='grey', ls='--')
            plt.plot([0, 0], [-100, 100], color='grey', ls='--')

            # Label the axes, with the percentage of variance explained
            plt.xlabel('PC{} ({}%)'.format(d1 + 1, round(200 * pca.explained_variance_ratio_[d1], 1)))
            plt.ylabel('PC{} ({}%)'.format(d2 + 1, round(200 * pca.explained_variance_ratio_[d2], 1)))

            plt.title("Projection of points (on PC{} and PC{})".format(d1 + 1, d2 + 1))
            plt.show(block=False)
            
def plot_dendrogram(Z, names, figsize=(10, 25)):
    '''Plot a dendrogram to illustrate hierarchical clustering'''

    plt.figure(figsize=figsize)
    plt.title('Hierarchical Clustering Dendrogram')
    plt.xlabel('distance')
    dendrogram(
        Z,
        labels=names,
        orientation="left",
        color_threshold=0
    )
    plt.show()

"""# Part one : Perform the clustering

## 1.   Data import

Before performing the data analysis, we need to import the data located in the file ***facilities.csv*** into a data frame.

*   Import the library **pandas** using **pd** as an alias to use its functions on the data.
*   Import the data from the file facilities.csv located in /content/sample_data/ into a data frame variable called ***raw_data_frame***.
"""

import pandas as pd

raw_data_frame = pd.read_csv(
    '/content/sample_data/facilities.csv')

"""## 2.   Data cleaning and normalization
*   Create a sub-set of the *raw_data_frame* that contains only the relevant columns for clustering: name it ***clutsering_frame***.
*   Import the class **StandardScaler**  and its method  **fit_transform** from the package **sklearn.preprocessing**
* Create an instance of the class StandardScaler that you call scaler.
*   Call the imported function **fit_transform**  to normalize the **clutsering_frame**: Assign the result to the clutsering_frame itself.

*   Create a new data frame named **normalized_frame** that contains the **clutsering_frame** as a parameter and **clutsering_frame.column** to create a new data frame of normlized data.

"""

clustering_frame = raw_data_frame[selected_clustering_variables]
clustering_frame = clustering_frame.fillna(clustering_frame.mean())
scaler = StandardScaler()
X_scaled = scaler.fit_transform(clustering_frame)

"""## 3.   Data clustering
After importing the data, selecting the relevant columns for the clustering, and normalizing them, our facilities are ready to be clustered.
First, import the class **AgglomerativeClustering** from the module **sklearn.cluster.**

* Create a hierarchical clustering model with the paramters:
  * **affinity**='euclidean'
  * **linkage**='ward'
  * **compute_full_tree**=True
  * **n_clusters**=1

* call the model to perform the clustering using the model on the data scaled data frame **normalized_frame** by calling the function **fit_predict**.


*   Import the package **numpy** with alias **np**.
*   Calculate the number of items per cluster using the function **bincount** of the NumPy package. The result is an array that you can store in a variable called **counts_per_cluster**.
"""

# Create a hierarchical clustering model
hier_cluster_model = AgglomerativeClustering(affinity='euclidean', linkage='ward', compute_full_tree=True)
# Fit the data to the model and determine which clusters each data point belongs to:
hier_cluster_model.set_params(n_clusters=n_clusters)
clusters = hier_cluster_model.fit_predict(X_scaled)
counts_per_cluster = (np.bincount(clusters))  # count of data points in each cluster

"""After performing the cluster on the subset of the data in the data frame  **normalized_frame**, we would like to add the cluster numbers to the original data. 
Remember that you only selected specific columns from the original data set. Now it's time to see in which cluster each facility belongs.


For possible external usage, you can export the **clustered_data** to a CSV file: it should contain all the columns from the **raw_data_frame** and an extra column called cluster - it has the cluster number. 
The facilites are exported to the file **clustered_facilities**.csv under `/content/sample_data/`
"""



# Add cluster number to the original data
X_scaled_clustered = pd.DataFrame(raw_data_frame, columns=raw_data_frame.columns, index=raw_data_frame.index)
X_scaled_clustered['cluster'] = clusters

X_scaled_clustered.head()
# export the data sets to csv file with cluster column
X_scaled_clustered.to_csv('/content/sample_data/clustered_facilities.csv', index=False)

"""# Part two: Visualise the clusters

After clustering our data, we would like to execute the PCA algorithm. It reduces all variables to two principial components. One of the merits: we can display the clustering results as a 2D plot.

* Show the clustering dendogram.
* Import the **PCA** class from the package **sklearn.decomposition **.
* Create a PCA model called **pca_cluserting** with *n_components=2.*
* Call the method **fit()** from the *pca_cluserting* to make the data frame fit into a model with only 2 components.
*   Call the method **transform()** from the *pca_cluserting* to transform the scaled data to the new PCA space.
* Dispaly the number of facilities per cluster.
"""

sample = clustering_frame
Z = linkage(clustering_frame, 'ward')
names = raw_data_frame['FACILITY_ID'].values
plot_dendrogram(Z, names, figsize=(20,20))

# Create a PCA model to reduce our data to 2 dimensions for visualisation
pca_cluserting = PCA(n_components=2)
pca_cluserting.fit(X_scaled)

# Transfor the scaled data to the new PCA space
X_reduced = pca_cluserting.transform(X_scaled)
display_factorial_planes(X_reduced, 2, pca_cluserting, [(0, 1)], illustrative_var=clusters, alpha=0.9)

#  print the  number of facilites per cluster 
dict_clust = dict(enumerate(counts_per_cluster.flatten(), 0))
for c in dict_clust:
    print('\ncluster {0} : {1} facilities.\n'.format(c, dict_clust.get(c)))